{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSrAxKeb9AxK",
        "outputId": "7f86b6a2-4313-410e-e7fd-15b80e6c4824"
      },
      "outputs": [],
      "source": [
        "!pip3 install snowflake-connector-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install scikit-learn numpy pandas matplotlib scipy seaborn plotly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYW2PiYmFU_R"
      },
      "outputs": [],
      "source": [
        "import configparser\n",
        "import snowflake.connector\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import mode\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data loading / Data Info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Description for Car Insurance Policy Price Prediction\n",
        "\n",
        "#### Overview:\n",
        "The dataset contains transaction histories of customers who purchased insurance policies. Each customer's entire quote history is recorded, with the last row indicating the purchased coverage options.\n",
        "\n",
        "#### Key Concepts:\n",
        "- **Customer**: May represent multiple individuals as policies can cover more than one person. Each customer has multiple shopping points (instances where they view products with specific characteristics and costs).\n",
        "- **Shopping Point**: Defined by a customer's interaction with a product at a specific time. Characteristics and product costs may change over time.\n",
        "- **Product Options**: Each product has 7 customizable options with 2-4 possible ordinal values.\n",
        "\n",
        "#### Variables:\n",
        "- **customer_ID**: Unique identifier for each customer.\n",
        "- **shopping_pt**: Unique identifier for the shopping point of a given customer.\n",
        "- **record_type**: Indicates whether the record is a shopping point (0) or purchase point (1).\n",
        "- **day**: Day of the week (0-6, 0=Monday).\n",
        "- **time**: Time of day (HH:MM).\n",
        "- **state**: State where the shopping point occurred.\n",
        "- **location**: Location ID of the shopping point.\n",
        "- **group_size**: Number of people covered under the policy (1-4).\n",
        "- **homeowner**: Homeownership status (0=no, 1=yes).\n",
        "- **car_age**: Age of the customer’s car.\n",
        "- **car_value**: Value of the customer’s car when new.\n",
        "- **risk_factor**: Risk assessment of the customer (1-4).\n",
        "- **age_oldest**: Age of the oldest person in the customer's group.\n",
        "- **age_youngest**: Age of the youngest person in the customer's group.\n",
        "- **married_couple**: Indicates if the customer group contains a married couple (0=no, 1=yes).\n",
        "- **C_previous**: Previous product option C/ Type of insured vehicle the customer had previously. (0=nothing, 1=Economy, 2=Mid-sized, 3=Luxury, 4=High-performance).\n",
        "- **duration_previous**: Duration (in years) the customer was covered by their previous insurer.\n",
        "\n",
        "#### Coverage Options:\n",
        "- **A**: Insurance coverage/risk profile (0=Basic, 1=Standard, 2=Premium).\n",
        "- **B**: Binary policyholder attribute (0=Non-smoker, 1=Smoker).\n",
        "- **C**: Type of insured vehicle (1=Economy, 2=Mid-sized, 3=Luxury, 4=High-performance).\n",
        "- **D**: Usage/purpose of the vehicle (1=Personal, 2=Business, 3=Commercial).\n",
        "- **E**: Vehicle safety features (0=No, 1=Yes).\n",
        "- **F**: Driver's record/history (0=Clean, 1=Minor violations, 2=Accidents, 3=Severe violations).\n",
        "- **G**: Geographical location/risk zone (1=Urban, 2=Suburban, 3=Rural, 4=Hazardous).\n",
        "\n",
        "#### Target Variable:\n",
        "- **cost**: Cost of the quoted coverage options.\n",
        "\n",
        "This dataset is used to predict the price of car insurance policies based on customer characteristics, product options, and interaction history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU5r2ZmU9Nov"
      },
      "outputs": [],
      "source": [
        "# Read the config file\n",
        "config = configparser.ConfigParser()\n",
        "config.read('config.ini')\n",
        "\n",
        "# Get the Snowflake credentials\n",
        "snowflake_config = config['snowflake']\n",
        "user = snowflake_config['user']\n",
        "password = snowflake_config['password']\n",
        "account = snowflake_config['account']\n",
        "warehouse = snowflake_config['warehouse']\n",
        "database = snowflake_config['database']\n",
        "schema = snowflake_config['schema']\n",
        "role = snowflake_config['role']\n",
        "\n",
        "# Step 1: Connect to Snowflake\n",
        "conn = snowflake.connector.connect(\n",
        "    user=user,\n",
        "    password=password,\n",
        "    account=account,\n",
        "    warehouse=warehouse,\n",
        "    database=database,\n",
        "    schema=schema,\n",
        "    role=role,\n",
        ")\n",
        "\n",
        "# Step 2: Execute SQL Query\n",
        "cur = conn.cursor()\n",
        "cur.execute('SELECT * FROM insurancetable')\n",
        "\n",
        "# Step 3: Fetch Data\n",
        "data = cur.fetchall()\n",
        "df = pd.DataFrame(data, columns=[x[0] for x in cur.description])\n",
        "\n",
        "# # Print DataFrame (optional)\n",
        "# print(df)\n",
        "# Step 4: Close the Connection\n",
        "cur.close()\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "NDKQFDWM9eb9",
        "outputId": "d800d79f-1ec2-4019-bef8-74253f69bd35"
      },
      "outputs": [],
      "source": [
        "# Display the first few rows of the dataset\n",
        "print(\"\\nFirst 5 rows of the dataset:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IqK1mED9ZEh",
        "outputId": "1a9d8b6f-c717-4f33-95ab-230bfd137f59"
      },
      "outputs": [],
      "source": [
        "# Display basic information about the dataset\n",
        "print(\"Dataset Information:\")\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "B_3qT69f9i_5",
        "outputId": "6ca8df81-c532-47e4-9f15-b6ac3b921c02"
      },
      "outputs": [],
      "source": [
        "# Summary statistics for numerical columns\n",
        "print(\"\\nSummary statistics for numerical columns:\")\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng0FDqpx9m4j",
        "outputId": "02e0077e-1abc-439e-d9dc-a0c17efd7fe9"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"\\nMissing values:\")\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hq-NVyWdFOnz"
      },
      "outputs": [],
      "source": [
        "# Replace empty strings with NaN\n",
        "df['STATE'].replace('', np.nan, inplace=True)\n",
        "df['CAR_VALUE'].replace('', np.nan, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dropping duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BptQnJ9DF0t",
        "outputId": "be521808-2f92-46ec-a3d0-f0ff067b2579"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo6SSc8EDAL1"
      },
      "outputs": [],
      "source": [
        "df.drop_duplicates(inplace =True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoKMwP2BF4XL",
        "outputId": "69befdcf-966f-4977-d820-4fc0985e30ca"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop rows where the 'STATE' column is 0 irrelevant values\n",
        "df = df[df['STATE'] != '0']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['STATE'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Missing Value Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0iUQXuQDbGL"
      },
      "outputs": [],
      "source": [
        "# Impute missing values\n",
        "def impute_grouped_data(df, column, method='mode'):\n",
        "    if method == 'mode':\n",
        "        df[column] = df.groupby('CUSTOMER_ID')[column].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else x))\n",
        "    elif method == 'median':\n",
        "        df[column] = df.groupby('CUSTOMER_ID')[column].transform(lambda x: x.fillna(x.median()))\n",
        "    elif method == 'mean':\n",
        "        df[column] = df.groupby('CUSTOMER_ID')[column].transform(lambda x: x.fillna(x.mean()))\n",
        "    elif method == 'ffill':\n",
        "        df[column] = df.groupby('CUSTOMER_ID')[column].transform(lambda x: x.fillna(method='ffill'))\n",
        "    elif method == 'bfill':\n",
        "        df[column] = df.groupby('CUSTOMER_ID')[column].transform(lambda x: x.fillna(method='bfill'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNdIvyx0D6Ky"
      },
      "outputs": [],
      "source": [
        "# Columns to impute and their methods\n",
        "columns_to_impute = {\n",
        "    'LOCATION': 'mode',\n",
        "    'GROUP_SIZE': 'mode',\n",
        "    'HOMEOWNER': 'mode',\n",
        "    'STATE': 'mode',\n",
        "    'CAR_VALUE': 'mode',\n",
        "    'CAR_AGE': 'mode',\n",
        "    'RISK_FACTOR': 'mode',\n",
        "    'AGE_OLDEST': 'mode',\n",
        "    'AGE_YOUNGEST': 'mode',\n",
        "    'MARRIED_COUPLE': 'mode',\n",
        "    'C_PREVIOUS': 'mode',\n",
        "    'DURATION_PREVIOUS': 'mode',\n",
        "    'A': 'mode',\n",
        "    'B': 'mode',\n",
        "    'C': 'mode',\n",
        "    'D': 'mode',\n",
        "    'E': 'mode',\n",
        "    'F': 'mode',\n",
        "    'G': 'mode'\n",
        "}\n",
        "\n",
        "# Apply imputation\n",
        "for column, method in columns_to_impute.items():\n",
        "    impute_grouped_data(df, column, method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rU_ZmZ__cMoQ"
      },
      "outputs": [],
      "source": [
        "# Drop rows where SHOPPING_PT is null because shopping point + customer_id is the unique identifier for each record \n",
        "df = df.dropna(subset=['SHOPPING_PT'])\n",
        "\n",
        "# Sort the rows based on CUSTOMER_ID and SHOPPING_PT\n",
        "df = df.sort_values(by=['CUSTOMER_ID', 'SHOPPING_PT'])\n",
        "\n",
        "# Correct the sequence of SHOPPING_PT to start from 1 for each CUSTOMER_ID\n",
        "df['SHOPPING_PT'] = df.groupby('CUSTOMER_ID').cumcount() + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RuYJH2ALqEp"
      },
      "outputs": [],
      "source": [
        "# Function to handle missing record_type values according to specified rules\n",
        "def fill_missing_record_type(group):\n",
        "    # Ensure group is sorted by 'SHOPPING_PT'\n",
        "    group = group.sort_values('SHOPPING_PT').reset_index(drop=True)\n",
        "\n",
        "    # Handle the last row separately\n",
        "    if pd.isnull(group['RECORD_TYPE'].iloc[-1]):\n",
        "        group['RECORD_TYPE'].iloc[-1] = 1\n",
        "\n",
        "    # Handle the rest of the rows\n",
        "    for i in range(len(group) - 1):\n",
        "        if pd.isnull(group['RECORD_TYPE'].iloc[i]):\n",
        "            group['RECORD_TYPE'].iloc[i] = 0\n",
        "\n",
        "    return group\n",
        "\n",
        "# Apply the function to each group of 'CUSTOMER_ID'\n",
        "df = df.groupby('CUSTOMER_ID', group_keys=False).apply(fill_missing_record_type)\n",
        "\n",
        "# Reset index\n",
        "df.reset_index(drop=True, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCxtNn1rTVJS"
      },
      "outputs": [],
      "source": [
        "def fill_missing_days(df):\n",
        "  # Forward fill missing values within each customer group\n",
        "  df['DAY'] = df.groupby('CUSTOMER_ID')['DAY'].ffill()\n",
        "\n",
        "  # Backward fill missing values within each customer group\n",
        "  df['DAY'] = df.groupby('CUSTOMER_ID')['DAY'].bfill()\n",
        "\n",
        "  # Handling edge cases of leading/trailing NaNs and isolated middle NaNs with different adjacent days\n",
        "  for customer in df['CUSTOMER_ID'].unique():\n",
        "    customer_data = df[df['CUSTOMER_ID'] == customer]\n",
        "\n",
        "    for i in range(1, len(customer_data) - 1):\n",
        "      if pd.isnull(customer_data.iloc[i]['DAY']):\n",
        "        prev_day = customer_data.iloc[i - 1]['DAY']\n",
        "        next_day = customer_data.iloc[i + 1]['DAY']\n",
        "        if prev_day != next_day:\n",
        "          # Fill with the most frequent day within the customer's data\n",
        "          most_frequent_day = customer_data['DAY'].mode().iloc[0]\n",
        "          df.loc[customer_data.index[i], 'DAY'] = most_frequent_day\n",
        "\n",
        "  return df\n",
        "\n",
        "df = fill_missing_days(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTzX4qXnEuon"
      },
      "outputs": [],
      "source": [
        "# Convert 'TIME' to datetime for easier manipulation\n",
        "df['TIME'] = pd.to_datetime(df['TIME'], format='%H:%M:%S', errors='coerce')\n",
        "\n",
        "# Function to handle missing time values according to specified rules\n",
        "def fill_missing_times(group):\n",
        "    # Ensure group is sorted by 'SHOPPING_PT'\n",
        "    group = group.sort_values('SHOPPING_PT')\n",
        "\n",
        "    n = len(group)\n",
        "\n",
        "    # Handle first row\n",
        "    if pd.isnull(group['TIME'].iloc[0]):\n",
        "        if n > 1:\n",
        "            group['TIME'].iloc[0] = group['TIME'].iloc[1] - pd.Timedelta(minutes=2)\n",
        "        else:\n",
        "            group['TIME'].iloc[0] = pd.Timestamp(group['DAY'].iloc[0]) + pd.Timedelta(hours=15, minutes=0, seconds=0)\n",
        "\n",
        "    # Handle middle rows\n",
        "    for i in range(1, n-1):\n",
        "        if pd.isnull(group['TIME'].iloc[i]):\n",
        "            if group['DAY'].iloc[i] == group['DAY'].iloc[i-1]:\n",
        "                group['TIME'].iloc[i] = group['TIME'].iloc[i-1] + pd.Timedelta(minutes=2)\n",
        "            elif group['DAY'].iloc[i] == group['DAY'].iloc[i+1]:\n",
        "                group['TIME'].iloc[i] = group['TIME'].iloc[i+1] - pd.Timedelta(minutes=2)\n",
        "\n",
        "    # Handle last row if more than one row exists\n",
        "    if n > 1 and pd.isnull(group['TIME'].iloc[-1]):\n",
        "        if group['DAY'].iloc[-1] == group['DAY'].iloc[-2]:\n",
        "            group['TIME'].iloc[-1] = group['TIME'].iloc[-2] + pd.Timedelta(minutes=2)\n",
        "        else:\n",
        "            group['TIME'].iloc[-1] = pd.Timestamp(group['DAY'].iloc[-1]) + pd.Timedelta(hours=15, minutes=0, seconds=0)\n",
        "\n",
        "    return group\n",
        "\n",
        "# Apply the function to each group of 'CUSTOMER_ID'\n",
        "df = df.groupby('CUSTOMER_ID', group_keys=False).apply(fill_missing_times)\n",
        "\n",
        "# Convert 'TIME' back to string format\n",
        "df['TIME'] = df['TIME'].dt.strftime('%H:%M:%S')\n",
        "\n",
        "# Reset index\n",
        "df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NyCDjxPe_Dc",
        "outputId": "1cbbca6f-6dc2-49b1-c535-134881629252"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"\\nMissing values:\")\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBvqF5LIenTC"
      },
      "outputs": [],
      "source": [
        "# Replacing the remaining missing values that are not imputed using mode \n",
        "# The reason may be not such car policy exists previously , hence filling them with 0\n",
        "df['C_PREVIOUS'].fillna(0, inplace=True)\n",
        "df['DURATION_PREVIOUS'].fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dropping the remaining null values except riskfactor\n",
        "df = df.dropna(subset = ['CAR_VALUE','GROUP_SIZE','HOMEOWNER','MARRIED_COUPLE','B','TIME'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEcnDGAkfCE4",
        "outputId": "0e497a4e-41a1-4bc1-ecef-0438fd7bab5e"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"\\nMissing values:\")\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# filtering Car_age column , removing outliers\n",
        "df = df[df['CAR_AGE'] <= 25]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['CAR_AGE'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvTJ7oFhikVB",
        "outputId": "a296f465-34ea-4263-adb7-0c5312ac520a"
      },
      "outputs": [],
      "source": [
        "df['RISK_FACTOR'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uehesRqY6k5E"
      },
      "outputs": [],
      "source": [
        "'''Imputation of null values in RISKFACTOR using K-Means Clustering'''\n",
        "# # Identify features to be used for clustering\n",
        "# features = ['HOMEOWNER', 'GROUP_SIZE', 'CAR_AGE', 'CAR_VALUE', 'AGE_OLDEST', 'AGE_YOUNGEST', 'MARRIED_COUPLE', 'COST']\n",
        "\n",
        "# # Drop rows where any feature for clustering is null, excluding RISK_FACTOR\n",
        "# X = df[features]\n",
        "# X = X.dropna(subset=features)\n",
        "\n",
        "# # Apply KMeans clustering\n",
        "# # scaler = StandardScaler()\n",
        "# # X = scaler.fit_transform(X.drop(columns=['CAR_VALUE']))\n",
        "\n",
        "# # Encode 'CAR_VALUE' after splitting and scaling\n",
        "# le_car_value = LabelEncoder()\n",
        "# X['CAR_VALUE'] = le_car_value.fit_transform(X['CAR_VALUE'].fillna(-1))\n",
        "\n",
        "# # Perform clustering\n",
        "# kmeans = KMeans(n_clusters=4, random_state=42)  # Number of clusters can be adjusted\n",
        "# clusters = kmeans.fit_predict(X)\n",
        "\n",
        "# Add clusters to the dataframe\n",
        "# df.loc[X.index, 'Cluster'] = clusters\n",
        "\n",
        "# # Function to impute missing RISK_FACTOR\n",
        "# def impute_risk_factor(row):\n",
        "#     if pd.isna(row['RISK_FACTOR']):\n",
        "#         cluster = row['Cluster']\n",
        "#         cluster_data = df[df['Cluster'] == cluster]['RISK_FACTOR'].dropna()\n",
        "#         if not cluster_data.empty:\n",
        "#             # Fix: Directly assign the mode value\n",
        "#             mode_value = mode(cluster_data).mode\n",
        "#         else:\n",
        "#             mode_value = 1  # Default value if no non-missing values are found in the cluster\n",
        "#         return mode_value\n",
        "#     else:\n",
        "#         return row['RISK_FACTOR']\n",
        "\n",
        "# # Apply imputation\n",
        "# df['RISK_FACTOR'] = df.apply(impute_risk_factor, axis=1)\n",
        "\n",
        "# # Ensure consistency within CUSTOMER_ID\n",
        "# customer_ids = df['CUSTOMER_ID'].unique()\n",
        "# for customer_id in customer_ids:\n",
        "#     customer_data = df[df['CUSTOMER_ID'] == customer_id]\n",
        "#     if customer_data['RISK_FACTOR'].isna().any():\n",
        "#         non_na_values = customer_data['RISK_FACTOR'].dropna()\n",
        "#         if not non_na_values.empty:\n",
        "#             # Fix: Directly assign the mode value\n",
        "#             mode_value = mode(non_na_values).mode\n",
        "#         else:\n",
        "#             mode_value = 1  # Default value if no non-missing values are found for the customer\n",
        "#         df.loc[df['CUSTOMER_ID'] == customer_id, 'RISK_FACTOR'] = mode_value\n",
        "\n",
        "# # Drop the Cluster column as it's no longer needed\n",
        "# df.drop(columns=['Cluster'], inplace=True)\n",
        "\n",
        "# print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4keT814N8iQM",
        "outputId": "4c1b673c-5dbd-4933-cb9d-7ad7ff3b98bb"
      },
      "outputs": [],
      "source": [
        "'''Imputation of null values in RISKFACTOR using prediction/ Random Forest Model'''\n",
        "\n",
        "# Separate rows with missing and non-missing RISK_FACTOR\n",
        "missing_risk_factor = df[df['RISK_FACTOR'].isna()]\n",
        "non_missing_risk_factor = df[~df['RISK_FACTOR'].isna()]\n",
        "\n",
        "# Select features for prediction\n",
        "features = ['HOMEOWNER', 'GROUP_SIZE', 'CAR_AGE', 'CAR_VALUE', 'AGE_OLDEST', 'AGE_YOUNGEST', 'MARRIED_COUPLE', 'COST']\n",
        "\n",
        "\n",
        "# Initialize a dictionary to store LabelEncoders\n",
        "label_encoders = {}\n",
        "\n",
        "# Label encode categorical variables\n",
        "for feature in ['CAR_VALUE', 'STATE']:\n",
        "    le = LabelEncoder()\n",
        "    non_missing_risk_factor[feature] = le.fit_transform(non_missing_risk_factor[feature].astype(str))\n",
        "    missing_risk_factor[feature] = le.transform(missing_risk_factor[feature].astype(str))\n",
        "\n",
        "    # Save the fitted LabelEncoder to dictionary\n",
        "    label_encoders[feature] = le\n",
        "\n",
        "# Train a model to predict RISK_FACTOR\n",
        "X = non_missing_risk_factor[features]\n",
        "y = non_missing_risk_factor['RISK_FACTOR']\n",
        "\n",
        "\n",
        "# Ensure y has no missing values and is of correct length\n",
        "assert len(X) == len(y), \"Mismatch in number of samples between X and y\"\n",
        "assert y.isna().sum() == 0, \"y contains missing values\"\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict missing RISK_FACTOR\n",
        "missing_X = missing_risk_factor[features]\n",
        "predicted_risk_factor = model.predict(missing_X)\n",
        "\n",
        "# Assign predicted values back to the dataset\n",
        "missing_risk_factor['RISK_FACTOR'] = predicted_risk_factor\n",
        "\n",
        "# Combine datasets\n",
        "df_dummy = pd.concat([non_missing_risk_factor, missing_risk_factor])\n",
        "\n",
        "# Ensure same RISK_FACTOR for each CUSTOMER_ID\n",
        "df['RISK_FACTOR'] = df_dummy.groupby('CUSTOMER_ID')['RISK_FACTOR'].transform(lambda x: x.mode()[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GIqJLNWL-CS",
        "outputId": "c988705e-51d9-419a-ed50-f4ef93a52469"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOlLK-jwNy8S",
        "outputId": "0d7f353d-500f-4f44-fc7d-b1ce095a6ce2"
      },
      "outputs": [],
      "source": [
        "df['STATE'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe-vblOeNwOs",
        "outputId": "8ff9e4b1-d515-41c9-f090-1d77a0ac25e3"
      },
      "outputs": [],
      "source": [
        "df['CAR_VALUE'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9iHJ2O7MKzz"
      },
      "outputs": [],
      "source": [
        "# Save or use the imputed dataset\n",
        "df.to_csv('preprocessed_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTlobTfsMGhZ",
        "outputId": "94c2dd59-b45d-48a7-a136-8f9865219082"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "2fYiK3LzWg9x",
        "outputId": "48a9bb18-94a7-4b31-e778-4b22ca20fd7a"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Loading the preprocessed data to snowflake then using it for visualiztion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from snowflake.connector.pandas_tools import write_pandas\n",
        "\n",
        "preprocessed_df = df\n",
        "preprocessed_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Writing the data to snowflake table\n",
        "\n",
        "# Read the config file\n",
        "config = configparser.ConfigParser()\n",
        "config.read('config.ini')\n",
        "\n",
        "# Get the Snowflake credentials\n",
        "snowflake_config = config['snowflake']\n",
        "user = snowflake_config['user']\n",
        "password = snowflake_config['password']\n",
        "account = snowflake_config['account']\n",
        "warehouse = snowflake_config['warehouse']\n",
        "database = snowflake_config['database']\n",
        "schema = snowflake_config['schema']\n",
        "role = snowflake_config['role']\n",
        "\n",
        "# Step 1: Connect to Snowflake\n",
        "conn = snowflake.connector.connect(\n",
        "    user=user,\n",
        "    password=password,\n",
        "    account=account,\n",
        "    warehouse=warehouse,\n",
        "    database=database,\n",
        "    schema=schema,\n",
        "    role=role,\n",
        ")\n",
        "\n",
        "# Write the DataFrame to the Snowflake table\n",
        "success, nchunks, nrows, _ = write_pandas(conn, preprocessed_df, 'PREPROCESSED_DATA')\n",
        "\n",
        "if success:\n",
        "    print(f\"Successfully wrote {nrows} rows in {nchunks} chunks to the Snowflake table 'preprocessed_data'.\")\n",
        "else:\n",
        "    print(\"Failed to write data to the Snowflake table.\")\n",
        "\n",
        "# Close the connection\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('preprocessed_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pairplot using seaborn to visualize relationships between variables\n",
        "sns.pairplot(df[['AGE_OLDEST', 'AGE_YOUNGEST', 'CAR_AGE', 'COST']])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotly interactive scatter plot\n",
        "fig = px.scatter(df, x='AGE_OLDEST', y='COST', color='RISK_FACTOR', \n",
        "                 title='Cost vs Age of Oldest Customer by Risk Factor')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of cost by risk factor using seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='RISK_FACTOR', y='COST', data=df)\n",
        "plt.title('Distribution of Cost by Risk Factor')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive plot for car value\n",
        "fig = px.histogram(df, x='CAR_VALUE', y='COST', color='CAR_VALUE', \n",
        "                   title='Cost Distribution by Car Value', barmode='group')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bar plot for coverage options using seaborn\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(x='A', data=df, palette='viridis')\n",
        "plt.title('Count of Coverage Option A')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive bar plot for STATE vs COST\n",
        "fig = px.bar(df, x='STATE', y='COST', color='STATE', title='Average Cost by State')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive line plot for time series analysis\n",
        "fig = px.line(df, x='TIME', y='COST', title='Cost Over Time')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Distribution of target column\n",
        "plt.hist(df['COST']); plt.title('Cost of Insurance Policy')\n",
        "plt.xlabel('Cost')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "id": "a5bb1USAtYv_",
        "outputId": "5358fc8c-3c78-49a1-e727-cf32aa5fbc62"
      },
      "outputs": [],
      "source": [
        "# Bivariate analysis: Correlation matrix and heatmap\n",
        "plt.figure(figsize=(20, 12))\n",
        "sns.heatmap(df.drop(['TIME','STATE','CAR_VALUE'], axis =1).corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ML Model Development "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Manually Selected features "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_features = ['STATE','GROUP_SIZE','HOMEOWNER','CAR_AGE','CAR_VALUE',\n",
        "                     'MARRIED_COUPLE','AGE_YOUNGEST','C_PREVIOUS','DURATION_PREVIOUS', 'A' ,'B','C','D','E','F' ,'G']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(selected_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Encoding Categorical Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize a dictionary to store LabelEncoders\n",
        "label_encoders = {}\n",
        "\n",
        "# Label encode categorical variables\n",
        "for feature in ['CAR_VALUE', 'STATE']:\n",
        "    le = LabelEncoder()\n",
        "    df[feature] = le.fit_transform(df[feature].astype(str))\n",
        "    # Save the fitted LabelEncoder to dictionary\n",
        "    label_encoders[feature] = le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['STATE'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['CAR_VALUE'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    # Define target variable\n",
        "target = 'COST'\n",
        "# Subset the data with selected features\n",
        "X_selected = df[selected_features]\n",
        "y = df[target]\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = trai_test_split(X_selected, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#X_selected.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#X_selected.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_selected.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_selected.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Columns to scale\n",
        "columns_to_scale = ['CAR_AGE', 'AGE_YOUNGEST']\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler on the training set and transform both training and testing sets\n",
        "X_train[columns_to_scale] = scaler.fit_transform(X_train[columns_to_scale])\n",
        "X_test[columns_to_scale] = scaler.transform(X_test[columns_to_scale])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model Training  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit the best model (Random Forest in this case) on the entire training data\n",
        "best_model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and evaluation on the training set\n",
        "train_predictions = best_model.predict(X_train)\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "train_rmse = np.sqrt(train_mse)\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "\n",
        "print(f\"Train Set Evaluation for Random Forest:\")\n",
        "print(f\"  MAE: {train_mae}\")\n",
        "print(f\"  MSE: {train_mse}\")\n",
        "print(f\"  RMSE: {train_rmse}\")\n",
        "print(f\"  R2: {train_r2}\")\n",
        "\n",
        "# Predictions and evaluation on the test set\n",
        "test_predictions = best_model.predict(X_test)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "test_r2 = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(f\"\\nTest Set Evaluation for Random Forest:\")\n",
        "print(f\"  MAE: {test_mae}\")\n",
        "print(f\"  MSE: {test_mse}\")\n",
        "print(f\"  RMSE: {test_rmse}\")\n",
        "print(f\"  R2: {test_r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataFrame with features, actual and predicted values for test set\n",
        "test_results = pd.DataFrame(X_test, columns=selected_features)\n",
        "test_results['Actual'] = y_test.values\n",
        "test_results['Predicted'] = test_predictions\n",
        "\n",
        "print(\"\\nSample of Test Results:\")\n",
        "print(test_results.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare metrics between train and test sets\n",
        "metrics_comparison = pd.DataFrame({\n",
        "    'Metric': ['MAE', 'MSE', 'RMSE', 'R2'],\n",
        "    'Train': [train_mae, train_mse, train_rmse, train_r2],\n",
        "    'Test': [test_mae, test_mse, test_rmse, test_r2]\n",
        "})\n",
        "\n",
        "# Save results to a CSV file\n",
        "metrics_comparison.to_csv('model_metrics.csv', index=True)\n",
        "print(\"\\nMetrics Comparison:\")\n",
        "metrics_comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Visualize the metrics comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Metric', y='value', hue='variable', data=pd.melt(metrics_comparison, ['Metric']))\n",
        "plt.title('Metrics Comparison between Train and Test Sets')\n",
        "plt.ylabel('Value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Saving the model for Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "# # Save the preprocessing steps and the model\n",
        "# # Save the LabelEncoders using joblib\n",
        "# for feature, encoder in label_encoders.items():\n",
        "#     joblib.dump(encoder, f'{feature}_label_encoder.pkl')\n",
        "joblib.dump(scaler, 'minmax_scaler.pkl')\n",
        "joblib.dump(model, 'random_forest_model.pkl')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "whIVGoWD5GxK",
        "ZfjYV4FwKklp",
        "jAertoIyv877",
        "JQ0xrIpyEsfe",
        "9gvNvUPJBmGa",
        "2GAWG0OgHUUk",
        "4blFKeK-44E9"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
